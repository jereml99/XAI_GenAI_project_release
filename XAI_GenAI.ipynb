{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Responsible AI: XAI GenAI project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Background\n",
    "\n",
    "\n",
    "\n",
    "Based on the previous lessons on explainability, post-hoc methods are used to explain the model, such as saliency map, SmoothGrad, LRP, LIME, and SHAP. Take LRP (Layer Wise Relevance Propagation) as an example; it highlights the most relevant pixels to obtain a prediction of the class \"cat\" by backpropagating the relevance. (image source: [Montavon et. al (2016)](https://giorgiomorales.github.io/Layer-wise-Relevance-Propagation-in-Pytorch/))\n",
    "\n",
    "<!-- %%[markdown] -->\n",
    "![LRP example](images/catLRP.jpg)\n",
    "\n",
    "Another example is about text sentiment classification, here we show a case of visualizing the importance of words given the prediction of 'positive':\n",
    "\n",
    "![text example](images/textGradL2.png)\n",
    "\n",
    "where the words highlight with darker colours indicate to be more critical in predicting the sentence to be 'positive' in sentiment.\n",
    "More examples could be found [here](http://34.160.227.66/?models=sst2-tiny&dataset=sst_dev&hidden_modules=Explanations_Attention&layout=default).\n",
    "\n",
    "Both cases above require the class or the prediction of the model. But:\n",
    "\n",
    "***How do you explain a model that does not predict but generates?***\n",
    "\n",
    "In this project, we will work on explaining the generative model based on the dependency between words. We will first look at a simple example, and using Point-wise Mutual Information (PMI) to compute the saliency map of the sentence. After that we will contruct the expereiment step by step, followed by exercises and questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A simple example to start with\n",
    "Given a sample sentence: \n",
    "> *Tokyo is the capital city of Japan.* \n",
    "\n",
    "We are going to explain this sentence by finding the dependency using a saliency map between words.\n",
    "The dependency of two words in the sentence could be measured by [Point-wise mutual information (PMI)](https://en.wikipedia.org/wiki/Pointwise_mutual_information): \n",
    "\n",
    "\n",
    "Mask two words out, e.g. \n",
    "> \\[MASK-1\\] is the captial city of \\[MASK-2\\].\n",
    "\n",
    "\n",
    "Ask the generative model to fill in the sentence 10 times, and we have:\n",
    "\n",
    "| MASK-1      | MASK-2 |\n",
    "| ----------- | ----------- |\n",
    "|    tokyo   |     japan   |\n",
    "|  paris  |     france    |\n",
    "|  london  |     england    |\n",
    "|  paris  |     france    |\n",
    "|  beijing |  china |\n",
    "|    tokyo   |     japan   |\n",
    "|  paris  |     france    |\n",
    "|  paris  |     france    |\n",
    "|  london  |     england    |\n",
    "|  beijing |  china |\n",
    "\n",
    "PMI is calculated by: \n",
    "\n",
    "$PMI(x,y)=log_2⁡ \\frac{p(\\{x,y\\}| s-\\{x,y\\})}{P(\\{x\\}|s-\\{x,y\\})P(\\{y\\}|s-\\{x,y\\})}$\n",
    "\n",
    "where $x$, $y$ represents the words that we masked out, $s$ represents the setence, and $s-\\{x,y\\}$ represents the sentences tokens after removing the words $x$ and $y$.\n",
    "\n",
    "In this example we have $PMI(Tokyo, capital) = log_2 \\frac{0.2}{0.2 * 0.2} = 2.32$\n",
    "\n",
    "Select an interesting word in the sentences; we can now compute the PMI between all other words and the chosen word using the generative model:\n",
    "(Here, we use a longer sentence and run 20 responses per word.)\n",
    "![](images/resPMI.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparation\n",
    "### 2.1 Conda enviroment\n",
    "\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "conda activate xai_llm\n",
    "```\n",
    "\n",
    "\n",
    "### 2.2 Download the offline LLM\n",
    "\n",
    "We use the offline LLM model from hugging face. It's approximately 5 GB.\n",
    "Download it using the comman below, and save it under `./models/`.\n",
    "```\n",
    "huggingface-cli download TheBloke/openchat-3.5-0106-GGUF openchat-3.5-0106.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n",
    "# credit to https://huggingface.co/TheBloke/openchat-3.5-0106-GGUF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mask the sentence and get the responses from LLM\n",
    "### 3.1 Get the input sentence\n",
    "\n",
    "**Remember to change the anchor word index when changing the input sentence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  tokyo is the capital of japan\n"
     ]
    }
   ],
   "source": [
    "def get_input():\n",
    "    # ideally this reads inputs from a file, now it just takes an input\n",
    "    return input(\"Enter a sentence: \")\n",
    "    \n",
    "anchor_word_idx = 0 # the index of the interested word\n",
    "prompts_per_word = 20 # number of generated responses  \n",
    "\n",
    "sentence =\"Tokyo is the capital of Japan\"\n",
    "sentence = sentence.lower()\n",
    "print(\"Sentence: \", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: openchat\n"
     ]
    }
   ],
   "source": [
    "from models.ChatModel import ChatModel\n",
    "model_name = \"openchat\"\n",
    "model = ChatModel(model_name)\n",
    "print(f\"Model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Run the prompts and get all the responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] [MASK] the capital of Japan: 100%|██████████| 20/20 [00:17<00:00,  1.12it/s]\n",
      "Input: [MASK] is [MASK] capital of Japan:  55%|█████▌    | 11/20 [00:10<00:07,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'is', '[mask]', 'capital', 'of', 'japan'] ['tokyo', 'is', 'japans', 'capital']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] is [MASK] capital of Japan: 100%|██████████| 20/20 [00:17<00:00,  1.14it/s]\n",
      "Input: [MASK] is the [MASK] of Japan: 100%|██████████| 20/20 [00:15<00:00,  1.30it/s]\n",
      "Input: [MASK] is the capital [MASK] Japan:   5%|▌         | 1/20 [00:00<00:12,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'is', 'the', 'capital', '[mask]', 'japan'] ['print']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] is the capital [MASK] Japan:  80%|████████  | 16/20 [00:11<00:02,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'is', 'the', 'capital', '[mask]', 'japan'] ['tokyo']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] is the capital [MASK] Japan:  95%|█████████▌| 19/20 [00:13<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'is', 'the', 'capital', '[mask]', 'japan'] ['tokyo', 'is', 'the', 'capital', '[tokyo]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] is the capital [MASK] Japan: 100%|██████████| 20/20 [00:14<00:00,  1.37it/s]\n",
      "Input: [MASK] is the capital of [MASK]:   5%|▌         | 1/20 [00:01<00:28,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Response is not valid. ['[mask]', 'is', 'the', 'capital', 'of', '[mask]'] ['须弥是日本的首都']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: [MASK] is the capital of [MASK]: 100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from tools.command_generator import generate_prompts, prefix_prompt\n",
    "from tools.evaluate_response import get_replacements\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_prompts(model, sentence, anchor_idx, prompts_per_word=20):\n",
    "    prompts = generate_prompts(sentence, anchor_idx)\n",
    "    all_replacements = []\n",
    "    for prompt in prompts:\n",
    "        replacements = []\n",
    "        for _ in tqdm(\n",
    "            range(prompts_per_word),\n",
    "            desc=f\"Input: {prompt}\",\n",
    "        ):\n",
    "            response = model.get_response(\n",
    "                prefix_prompt(prompt),\n",
    "            ).strip()\n",
    "            if response:\n",
    "                replacement = tuple(get_replacements(prompt, response))\n",
    "                if replacement:\n",
    "                    replacements.append(replacement)\n",
    "        if len(replacements) > 0:\n",
    "            all_replacements.append(replacements)\n",
    "    return all_replacements\n",
    "\n",
    "all_responses = run_prompts(model, sentence, anchor_word_idx, prompts_per_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 EXERCISE: compute the PMI for each word\n",
    "\n",
    "$PMI(x,y)=log_2⁡ \\frac{p(\\{x,y\\}| s-\\{x,y\\})}{P(\\{x\\}|s-\\{x,y\\})P(\\{y\\}|s-\\{x,y\\})}$\n",
    "\n",
    "* Compute the $P(x)$, $P(y)$ and $P(x,y)$ first and print it out.\n",
    "* Compute the PMI for each word.\n",
    "* Visualize the result by coloring. Tips: you might need to normalize the result first. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: [MASK] [MASK] the capital of Japan\n",
      "('\\x0e', 'tokyo is')\n",
      "('tokyo', 'is')\n",
      "('tokyo', 'is')\n",
      "('tokyo', 'is')\n",
      "('tokyo', 'is')\n",
      "('the', 'city of tokyo is')\n",
      "('tokyo', 'is')\n",
      "('tokyo', 'is')\n",
      "('tokyo', 'is')\n",
      "('tokyo', 'is')\n",
      "('tokyo', 'is')\n",
      "('tokyo', 'is')\n",
      "('tokyo', 'is')\n",
      "('tokyo', 'is')\n",
      "('tokyo', 'is')\n",
      "('tokyo', 'is')\n",
      "('tokyo', 'is')\n",
      "('tokyo', 'is')\n",
      "('tokyo', 'is')\n",
      "('tokyo', 'is')\n",
      "\n",
      "\n",
      "Prompt: [MASK] is [MASK] capital of Japan\n",
      "('chrome', 'not the')\n",
      "('tokyo', 'the')\n",
      "('tokyo', 'the')\n",
      "('tokyo', 'the')\n",
      "('tokyo', 'the')\n",
      "('tokyo', 'the')\n",
      "('tokyo', 'the')\n",
      "('tokyo', 'the')\n",
      "('tokyo', 'tokyos')\n",
      "('tokyo', 'the')\n",
      "('', '')\n",
      "('tokyo', 'the')\n",
      "('tokyo', 'tokyos')\n",
      "('tokyo', 'the')\n",
      "('tokyo', 'the')\n",
      "('tokyo', 'the')\n",
      "('tokyo', 'the')\n",
      "('tokyo', 'the')\n",
      "('tokyo', 'the')\n",
      "('tokyo', 'the')\n",
      "\n",
      "\n",
      "Prompt: [MASK] is the [MASK] of Japan\n",
      "('opyright', 'cultural property')\n",
      "('tokyo', 'capital')\n",
      "('tokyo', 'capital')\n",
      "('tokyo', 'capital')\n",
      "('tokyo', 'capital')\n",
      "('tokyo', 'capital')\n",
      "('tokyo', 'capital')\n",
      "('tokyo', 'capital')\n",
      "('tokyo', 'capital')\n",
      "('tokyo', 'capital')\n",
      "('emperor', 'monarch')\n",
      "('tokyo', 'capital')\n",
      "('tokyo', 'capital')\n",
      "('tokyo', 'capital')\n",
      "('tokyo', 'capital')\n",
      "('tokyo', 'capital')\n",
      "('emperor', 'title')\n",
      "('tokyo', 'capital')\n",
      "('tokyo', 'capital')\n",
      "('tokyo', 'capital')\n",
      "\n",
      "\n",
      "Prompt: [MASK] is the capital [MASK] Japan\n",
      "('', '')\n",
      "('tokyo', 'of')\n",
      "('tokyo', 'of')\n",
      "('tokyo', 'of')\n",
      "('tokyo', 'of')\n",
      "('tokyo', 'of')\n",
      "('tokyo', 'of')\n",
      "('tokyo', '[mask]')\n",
      "('tokyo', 'of')\n",
      "('tokyo', 'of')\n",
      "('tokyo', 'of')\n",
      "('tokyo', 'of')\n",
      "('tokyo', 'of')\n",
      "('tokyo', 'of')\n",
      "('tokyo', 'of')\n",
      "('', '')\n",
      "('tokyo', 'of')\n",
      "('tokyo', 'of')\n",
      "('', '')\n",
      "('tokyo', 'city of')\n",
      "\n",
      "\n",
      "Prompt: [MASK] is the capital of [MASK]\n",
      "('', '')\n",
      "('paris', 'france')\n",
      "('paris', 'france')\n",
      "('paris', 'france')\n",
      "('paris', 'france')\n",
      "('tokyo', 'japan')\n",
      "('tokyo', 'japan')\n",
      "('london', 'the united kingdom')\n",
      "('paris', 'france')\n",
      "('tokyo', 'japan')\n",
      "('london', 'the united kingdom')\n",
      "('paris', 'france')\n",
      "('paris', 'france')\n",
      "('paris', 'france')\n",
      "('paris', 'france')\n",
      "('london', 'england')\n",
      "('paris', 'france')\n",
      "('tokyo', 'japan')\n",
      "('london', 'the united kingdom')\n",
      "('paris', 'france')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, replacements in enumerate(all_responses):\n",
    "    print(f\"Prompt: {generate_prompts(sentence, anchor_word_idx)[i]}\")\n",
    "    for replacement in replacements:\n",
    "        print(replacement)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': np.float64(0.15200309326693948),\n",
       " 'the': np.float64(0.15200309324467542),\n",
       " 'capital': np.float64(0.234465253437342),\n",
       " 'of': np.float64(0.23446525341071792),\n",
       " 'japan': np.float64(2.3219280912806246)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def caluclate_pmi(sentence, anchor_word_idx, all_responses):\n",
    "    anchor_word = sentence.split()[anchor_word_idx]\n",
    "    pmi_dict = {}\n",
    "    \n",
    "    i = 0\n",
    "    for word in sentence.split():\n",
    "        if word != anchor_word:\n",
    "            frequency_of_pairs = Counter(all_responses[i])\n",
    "            frequency_of_X = Counter([pair[0] for pair in all_responses[i]])\n",
    "            frequency_of_Y = Counter([pair[1] for pair in all_responses[i]])\n",
    "            \n",
    "            p_x = frequency_of_X[anchor_word] / len(all_responses[i])\n",
    "            p_y = frequency_of_Y[word] / len(all_responses[i])\n",
    "            p_xy = frequency_of_pairs[(anchor_word, word)] / len(all_responses[i])\n",
    "            epsilon = 1e-10\n",
    "            pmi = np.log2(p_xy / (p_x * p_y + epsilon))\n",
    "            pmi_dict[word] = pmi\n",
    "            i += 1\n",
    "    return pmi_dict\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_pmi(pmi_dict):\n",
    "    words = list(pmi_dict.keys())\n",
    "    pmi_values = np.array(list(pmi_dict.values()))\n",
    "    \n",
    "    # Normalize the PMI values\n",
    "    norm = plt.Normalize(pmi_values.min(), pmi_values.max())\n",
    "    norm_pmi_values = norm(pmi_values)\n",
    "    \n",
    "    # Create a color map\n",
    "    cmap = plt.get_cmap('coolwarm')\n",
    "    colors = cmap(norm_pmi_values)\n",
    "    \n",
    "    # Plot the words with colors based on PMI values\n",
    "    plt.figure(figsize=(10, 1))\n",
    "    for i, word in enumerate(words):\n",
    "        plt.text(i, 0.5, word, fontsize=12, ha='center', va='center', color=colors[i])\n",
    "    \n",
    "    plt.xlim(-1, len(words))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "pmi_dict = caluclate_pmi(sentence, anchor_word_idx, all_responses)\n",
    "visualize_pmi(pmi_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. EXERCISE: Try more examples; maybe come up with your own. Report the results.\n",
    "\n",
    "* Try to come up with more examples and, change the anchor word/number of responses, and observe the results. What does the explanation mean? Do you think it's a nice explanation? Why and why not? \n",
    "* What's the limitation of the current method? When does the method fail to explain? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bonus Exercises\n",
    "### 5.1 Language pre-processing. \n",
    "In this exercise, we only lower the letters and split sentences into words; there's much more to do to pre-process the language. For example, contractions (*I'll*, *She's*, *world's*), suffix and prefix, compound words (*hard-working*). It's called word tokenization in NLP, and there are some Python packages that can do such work for us, e.g. [*TextBlob*](https://textblob.readthedocs.io/en/dev/). \n",
    "\n",
    "\n",
    "### 5.2 Better word matching\n",
    "In the above example of\n",
    "> Tokyo is the capital of Japan and a popular metropolis in the world.\n",
    "\n",
    ", GenAI never gives the specific word 'metropolis' when masking it out; instead, sometimes it provides words like 'city', which is not the same word but has a similar meaning. Instead of measuring the exact matching of certain words (i.e. 0 or 1), we can also measure the similarity of two words, e.g. the cosine similarity in word embedding, which ranges from 0 to 1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
